{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf36f75",
   "metadata": {},
   "source": [
    "Sara Hernández Ochoa "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e2d3f",
   "metadata": {},
   "source": [
    "# Teorico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604fa40",
   "metadata": {},
   "source": [
    "#### Solo las respuestas incorrectas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d0a9f",
   "metadata": {},
   "source": [
    "1. Explica el modelo de regresión logística para clasificación. ¿Cómo se determina el umbral de decisión?\n",
    "\n",
    "El modelo de regresión logística es un modelo estadístico utilizado para problemas de clasificación binaria. Estima la probabilidad de que ocurra un determinado evento, por ejemplo, que un cliente decida comprar un producto o no, a partir de un conjunto de variables independientes. El umbral de decisión se determina mediante el método de máxima verosimilitud, buscando los valores que hacen más probable observar los datos disponibles.\n",
    "\n",
    "2. Explica la intuición de la máquina de soporte vectorial para clasificación. ¿Cómo se determina qué modelo es mejor? ¿Cuál es la mayor diferencia que tiene contra un modelo de regresión logística?\n",
    "\n",
    "Una máquina de soporte vectorial (SVM) trata de encontrar la línea o superficie que mejor separa las clases de datos, dejando el mayor espacio posible entre ellas. Por ejemplo, puede usarse para distinguir correos electrónicos “spam” de los que no lo son, analizando características como palabras clave o frecuencia de enlaces. El mejor modelo se elige probando diferentes configuraciones y quedándose con el que clasifica mejor los datos de prueba, normalmente usando validación cruzada. La principal diferencia con la regresión logística es que la SVM se enfoca en separar las clases con el mayor margen posible, mientras que la regresión logística busca predecir la probabilidad de que un dato pertenezca a una clase.\n",
    "\n",
    "3. ¿Cuáles son los componentes principales en un MLP para clasificación? Dibuja un ejemplo y señaliza.\n",
    "\n",
    "CORRECTA\n",
    "\n",
    "4. ¿Cuál es el procedimiento a seguir cuando los datos no son linealmente separables en una SVC?\n",
    "\n",
    "CORRECTA\n",
    "\n",
    "5. Describe qué es un hiperparámetro. ¿Por qué es importante ajustarlos? Da dos ejemplos de hiperparámetros.\n",
    "\n",
    "Un hiperparámetro es un valor o conjunto de valores que se define antes de entrenar un modelo y que controla su comportamiento o estructura. Estos valores no se aprenden de los datos, sino que se establecen para guiar el proceso de entrenamiento. Ajustarlos correctamente es clave porque afectan el rendimiento y la capacidad del modelo para generalizar. Un mal ajuste puede hacer que el modelo aprenda de forma ineficiente o se sobreajuste. Dos ejemplos de hiperparámetros son la tasa de aprendizaje en un modelo de gradiente descendente y la profundidad máxima de un árbol de decisión.\n",
    "\n",
    "6. Dibuja un diagrama de flujo para describir el proceso de optimización Bayesiana.\n",
    "\n",
    "CORRECTA\n",
    "\n",
    "7. ¿Qué es la curva ROC y cómo se usa para evaluar el desempeño de un modelo?\n",
    "\n",
    "La curva ROC, Receiver Operating Characteristic, es una gráfica que muestra el desempeño de un modelo de clasificación binaria al variar el umbral de decisión. En el eje X se representa la tasa de falsos positivos (FPR) y en el eje Y la tasa de verdaderos positivos (TPR). Por ejemplo, en un modelo que predice si una transacción es fraudulenta o no, cada punto de la curva corresponde a un valor distinto del umbral usado para decidir si una operación se considera sospechosa. Al mover el umbral de 0 a 1, el modelo genera diferentes pares de valores (FPR, TPR) que forman la curva completa. Para evaluar su desempeño se calcula el área bajo la curva (AUC); cuanto más cercana a 1 sea esta, mejor será la capacidad del modelo para distinguir entre las clases, mientras que un valor de 0.5 indica un modelo que clasifica al azar.\n",
    "\n",
    "8. Describe un espacio de Hilbert.\n",
    "\n",
    "El espacio de Hilbert representa un espacio vectorial de dimensión infinita al que se transforman los datos para facilitar su separación. En otras palabras, es un espacio más grande o con más dimensiones donde los datos que antes no podían separarse con una línea o plano ahora sí pueden distinguirse mejor. Por ejemplo, en un modelo que intenta clasificar flores según su forma y color, al llevar los datos al espacio de Hilbert, se pueden separar especies que antes se traslapaban en el espacio original.\n",
    "\n",
    "9. ¿Qué significa que una función de costo sea convexa? ¿Qué beneficios hay de que un modelo tenga una función de costo convexa?\n",
    "\n",
    "CORRECTA\n",
    "\n",
    "10. Piensa en los 3 modelos aprendidos en este parcial: ¿En qué situaciones usarías cada uno y por qué?\n",
    "\n",
    "Regresión Logística: La emplearía en problemas de clasificación binaria donde los datos se puedan separar de forma lineal o casi lineal. Es un modelo rápido, fácil de interpretar y sencillo de implementar. Por ejemplo, serviría para estimar si una persona padece o no diabetes a partir de sus características.\n",
    "\n",
    "SVC: La utilizaría cuando los datos no se puedan separar linealmente o presenten una estructura más compleja, aprovechando la capacidad de los kernels para transformar el espacio de los datos. Por ejemplo, podría aplicarse para clasificar distintos tipos de imágenes.\n",
    "\n",
    "MLP: La usaría en casos no lineales y de mayor complejidad, especialmente cuando se dispone de una gran cantidad de datos. Este modelo puede captar relaciones más profundas entre las variables. Por ejemplo, sería útil para identificar patrones en imágenes, sonidos o textos, donde intervienen muchos factores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92ed2a0",
   "metadata": {},
   "source": [
    "# Examen 2 - Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3847cd",
   "metadata": {},
   "source": [
    "## Pregunta 1"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "21dfd1bf",
   "metadata": {},
   "source": [
    "Utiliza optimización bayesiana para intentar encontrar el mínimo global de la siguiente función:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "donde las 3 variables están acotadas en [0, 1]. Crea 5 muestras iniciales e itera 15 veces para optimizar.\n",
    "\n",
    " \n",
    "\n",
    "Explica por qué optimización bayesiana es una elección buena para este problema en lugar de GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "076b168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cda35131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración 1: Nuevo punto = 0.7487, Valor función = -10.2936\n",
      "Iteración 2: Nuevo punto = 0.7718, Valor función = -8.1970\n",
      "Iteración 3: Nuevo punto = 0.7708, Valor función = -8.3160\n",
      "Iteración 4: Nuevo punto = 0.0000, Valor función = 3.4398\n",
      "Iteración 5: Nuevo punto = 0.0010, Valor función = 3.3204\n",
      "Iteración 6: Nuevo punto = 0.0020, Valor función = 3.2017\n",
      "Iteración 7: Nuevo punto = 0.0030, Valor función = 3.0838\n",
      "Iteración 8: Nuevo punto = 0.0040, Valor función = 2.9667\n",
      "Iteración 9: Nuevo punto = 0.0050, Valor función = 2.8505\n",
      "Iteración 10: Nuevo punto = 0.0060, Valor función = 2.7351\n",
      "Iteración 11: Nuevo punto = 0.0070, Valor función = 2.6205\n",
      "Iteración 12: Nuevo punto = 0.7337, Valor función = -10.9612\n",
      "Iteración 13: Nuevo punto = 0.0080, Valor función = 2.5069\n",
      "Iteración 14: Nuevo punto = 0.0090, Valor función = 2.3941\n",
      "Iteración 15: Nuevo punto = 0.0100, Valor función = 2.2822\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from scipy.stats import norm\n",
    "\n",
    "def iteracion(funcion, X, Y, Z, x, y, z, n_iter=5, tipo=\"reducir incertidumbre\", nivel_conf=0.95):\n",
    "    X = np.asarray(X).reshape(-1, 1)\n",
    "    Y = np.asarray(Y).reshape(-1, 1)\n",
    "    Z = np.asarray(Z).reshape(-1, 1)\n",
    "    x = np.asarray(x).reshape(-1, 1)\n",
    "    y = np.asarray(y).reshape(-1, 1)\n",
    "    z = np.asarray(z).reshape(-1, 1)\n",
    "\n",
    "    f_vals = funcion(X, Y, Z).ravel()\n",
    "\n",
    "    kernel = 1.0 * RBF(length_scale=1.0)\n",
    "    gp = GPR(kernel=kernel, n_restarts_optimizer=10)\n",
    "    gp.fit(X, f_vals)\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        y_pred, std = gp.predict(x, return_std=True)\n",
    "        y_pred = y_pred.ravel()\n",
    "\n",
    "        nc = norm.ppf(1 - (1 - nivel_conf) / 2)\n",
    "        y_upper = y_pred + nc * std\n",
    "        y_lower = y_pred - nc * std\n",
    "\n",
    "        if tipo == \"reducir incertidumbre\":\n",
    "            index = np.argmax(std)\n",
    "        elif tipo == \"maximizar\":\n",
    "            index = np.argmax(y_upper)\n",
    "        elif tipo == \"minimizar\":\n",
    "            index = np.argmin(y_lower)\n",
    "\n",
    "        x_val = x[index].reshape(1, -1)\n",
    "        y_val = funcion(x_val, x_val, x_val).ravel()[0] \n",
    "\n",
    "        X = np.vstack([X, x_val])\n",
    "        f_vals = np.concatenate([f_vals, np.array([y_val])])\n",
    "\n",
    "        gp.fit(X, f_vals)\n",
    "\n",
    "        print(f\"Iteración {i + 1}: Nuevo punto = {x_val.ravel()[0]:.4f}, Valor función = {y_val:.4f}\")\n",
    "\n",
    "# Funcion objetivo\n",
    "def f(x, y, z):\n",
    "    return (6*x - 2)**2 * np.sin(12*x - 4) + (6*y - 2)**2 * np.cos(12*y - 4) + (6*z - 2)**2 * np.sin(12*z - 4)\n",
    "\n",
    "# 5 muestras iniciales aleatorias\n",
    "np.random.seed(10)\n",
    "X = np.random.uniform(0, 1, 5).reshape([-1, 1])\n",
    "Y = np.random.uniform(0, 1, 5).reshape([-1, 1])\n",
    "Z = np.random.uniform(0, 1, 5).reshape([-1, 1])\n",
    "\n",
    "#Búsqueda\n",
    "x = np.linspace(0, 1, 1000).reshape([-1, 1])\n",
    "y = np.linspace(0, 1, 1000).reshape([-1, 1])\n",
    "z = np.linspace(0, 1, 1000).reshape([-1, 1])\n",
    "\n",
    "\n",
    "iteracion(f, X, Y, Z, x, y, z, n_iter=15, tipo=\"minimizar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece949b",
   "metadata": {},
   "source": [
    "Usando optimización bayesiana conseguimos encontrar rápido el mínimo de la función, llegando al valor más bajo en la iteración 12 con x = 0.7337 y función = –10.9612."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89067f5",
   "metadata": {},
   "source": [
    "## Pregunta 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e1420",
   "metadata": {},
   "source": [
    "Utiliza el modelo de tu elección, con los hiperparámetros y factores de tu elección para obtener el mejor predictor que puedas conseguir para decidir si un paciente del dataset padece diabetes.\n",
    "\n",
    "Cross-validation a utilizar: K-folds.\n",
    "\n",
    "Métrico a utilizar: Precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c749d07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.639947</td>\n",
       "      <td>0.848324</td>\n",
       "      <td>0.149641</td>\n",
       "      <td>0.907270</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.468492</td>\n",
       "      <td>1.425995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-1.123396</td>\n",
       "      <td>-0.160546</td>\n",
       "      <td>0.530902</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.684422</td>\n",
       "      <td>-0.365061</td>\n",
       "      <td>-0.190672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.233880</td>\n",
       "      <td>1.943724</td>\n",
       "      <td>-0.263941</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-1.103255</td>\n",
       "      <td>0.604397</td>\n",
       "      <td>-0.105584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-0.998208</td>\n",
       "      <td>-0.160546</td>\n",
       "      <td>0.154533</td>\n",
       "      <td>0.123302</td>\n",
       "      <td>-0.494043</td>\n",
       "      <td>-0.920763</td>\n",
       "      <td>-1.041549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.141852</td>\n",
       "      <td>0.504055</td>\n",
       "      <td>-1.504687</td>\n",
       "      <td>0.907270</td>\n",
       "      <td>0.765836</td>\n",
       "      <td>1.409746</td>\n",
       "      <td>5.484909</td>\n",
       "      <td>-0.020496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>1.827813</td>\n",
       "      <td>-0.622642</td>\n",
       "      <td>0.356432</td>\n",
       "      <td>1.722735</td>\n",
       "      <td>0.870031</td>\n",
       "      <td>0.115169</td>\n",
       "      <td>-0.908682</td>\n",
       "      <td>2.532136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>-0.547919</td>\n",
       "      <td>0.034598</td>\n",
       "      <td>0.046245</td>\n",
       "      <td>0.405445</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.610154</td>\n",
       "      <td>-0.398282</td>\n",
       "      <td>-0.531023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.342981</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>0.149641</td>\n",
       "      <td>0.154533</td>\n",
       "      <td>0.279594</td>\n",
       "      <td>-0.735190</td>\n",
       "      <td>-0.685193</td>\n",
       "      <td>-0.275760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>0.159787</td>\n",
       "      <td>-0.470732</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.240205</td>\n",
       "      <td>-0.371101</td>\n",
       "      <td>1.170732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-0.873019</td>\n",
       "      <td>0.046245</td>\n",
       "      <td>0.656358</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.202129</td>\n",
       "      <td>-0.473785</td>\n",
       "      <td>-0.871374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       "0       0.639947  0.848324       0.149641       0.907270 -0.692891  0.204013   \n",
       "1      -0.844885 -1.123396      -0.160546       0.530902 -0.692891 -0.684422   \n",
       "2       1.233880  1.943724      -0.263941      -1.288212 -0.692891 -1.103255   \n",
       "3      -0.844885 -0.998208      -0.160546       0.154533  0.123302 -0.494043   \n",
       "4      -1.141852  0.504055      -1.504687       0.907270  0.765836  1.409746   \n",
       "..           ...       ...            ...            ...       ...       ...   \n",
       "763     1.827813 -0.622642       0.356432       1.722735  0.870031  0.115169   \n",
       "764    -0.547919  0.034598       0.046245       0.405445 -0.692891  0.610154   \n",
       "765     0.342981  0.003301       0.149641       0.154533  0.279594 -0.735190   \n",
       "766    -0.844885  0.159787      -0.470732      -1.288212 -0.692891 -0.240205   \n",
       "767    -0.844885 -0.873019       0.046245       0.656358 -0.692891 -0.202129   \n",
       "\n",
       "     DiabetesPedigreeFunction       Age  \n",
       "0                    0.468492  1.425995  \n",
       "1                   -0.365061 -0.190672  \n",
       "2                    0.604397 -0.105584  \n",
       "3                   -0.920763 -1.041549  \n",
       "4                    5.484909 -0.020496  \n",
       "..                        ...       ...  \n",
       "763                 -0.908682  2.532136  \n",
       "764                 -0.398282 -0.531023  \n",
       "765                 -0.685193 -0.275760  \n",
       "766                 -0.371101  1.170732  \n",
       "767                 -0.473785 -0.871374  \n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('diabetes.csv')\n",
    "y = data['Outcome']\n",
    "X = data.drop(columns=['Outcome'])\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = pd.DataFrame(scaler.transform(X), columns=X.columns)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c17f644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression: Precision = 0.7295 ± 0.1264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr_results': {'mean': 0.7295172219085262,\n",
       "  'std': 0.12640381775541584,\n",
       "  'predictions': array([0.71981638, 0.04933123, 0.79421539, 0.04226898, 0.90041553,\n",
       "         0.14753243, 0.06731905, 0.64097778, 0.7097593 , 0.03721347,\n",
       "         0.22044378, 0.89587106, 0.78299976, 0.63136585, 0.62706761,\n",
       "         0.39906913, 0.37177202, 0.19736995, 0.35667009, 0.23497608,\n",
       "         0.39290264, 0.31760973, 0.93916912, 0.29478265, 0.70111706,\n",
       "         0.44118653, 0.7353875 , 0.04651918, 0.5392435 , 0.27861723,\n",
       "         0.42608398, 0.57257665, 0.04967327, 0.03712914, 0.43217669,\n",
       "         0.15065195, 0.66342646, 0.39336248, 0.17298955, 0.5723509 ,\n",
       "         0.74155082, 0.69469157, 0.11492119, 0.92606855, 0.62715685,\n",
       "         0.95124578, 0.43321492, 0.04038956, 0.38030842, 0.03940934,\n",
       "         0.03682523, 0.08594148, 0.06815446, 0.82610491, 0.70780091,\n",
       "         0.02307931, 0.8817425 , 0.35816004, 0.83150367, 0.18399248,\n",
       "         0.01011081, 0.52045455, 0.02385955, 0.3085604 , 0.35427064,\n",
       "         0.11950116, 0.1934116 , 0.47113114, 0.03254582, 0.30235727,\n",
       "         0.19435476, 0.36412302, 0.81553638, 0.2473451 , 0.05229664,\n",
       "         0.00203153, 0.07719661, 0.22264235, 0.66849353, 0.10099177,\n",
       "         0.10304983, 0.00601279, 0.13816207, 0.05071594, 0.67424721,\n",
       "         0.19287928, 0.52880672, 0.1877996 , 0.78310101, 0.07509597,\n",
       "         0.01963785, 0.25928607, 0.33143355, 0.29688766, 0.25522516,\n",
       "         0.51549517, 0.08263986, 0.01765711, 0.134758  , 0.45204085,\n",
       "         0.83485704, 0.28223551, 0.06336086, 0.03234252, 0.23167185,\n",
       "         0.2473727 , 0.01992434, 0.42494587, 0.10747755, 0.09396762,\n",
       "         0.59968385, 0.70680427, 0.05470041, 0.09417003, 0.73860716,\n",
       "         0.55332091, 0.35675798, 0.15874376, 0.11886339, 0.05344606,\n",
       "         0.88466872, 0.28481945, 0.14988301, 0.35468975, 0.13665439,\n",
       "         0.52528797, 0.45764913, 0.19556077, 0.19544672, 0.15569068,\n",
       "         0.63160321, 0.68322478, 0.67171   , 0.2728204 , 0.05564973,\n",
       "         0.24322143, 0.09848338, 0.0685107 , 0.26202433, 0.17713753,\n",
       "         0.16850189, 0.32005654, 0.17295495, 0.38572039, 0.46655651,\n",
       "         0.00725049, 0.0644852 , 0.30267378, 0.63088768, 0.04550641,\n",
       "         0.35298459, 0.13753085, 0.84450412, 0.52286488, 0.9605947 ,\n",
       "         0.87861362, 0.0878376 , 0.12534199, 0.05023458, 0.96973691,\n",
       "         0.4255449 , 0.30219561, 0.22335549, 0.10457051, 0.27835554,\n",
       "         0.23416478, 0.44635538, 0.31755737, 0.23481232, 0.12385192,\n",
       "         0.15543692, 0.52232081, 0.19903154, 0.19072772, 0.05203983,\n",
       "         0.87217757, 0.12802155, 0.77515694, 0.75158936, 0.65594286,\n",
       "         0.04668114, 0.26687572, 0.00210051, 0.04989806, 0.34622475,\n",
       "         0.94790489, 0.8381326 , 0.37372773, 0.24316848, 0.35765151,\n",
       "         0.07778729, 0.48210011, 0.68668627, 0.97054858, 0.10004493,\n",
       "         0.68066726, 0.06411867, 0.10903359, 0.34112956, 0.37279491,\n",
       "         0.17343958, 0.42891336, 0.13909586, 0.03993994, 0.33295461,\n",
       "         0.13139801, 0.9485112 , 0.69366492, 0.0935751 , 0.87469974,\n",
       "         0.04971967, 0.55210567, 0.83368662, 0.52359534, 0.31360584,\n",
       "         0.89612807, 0.30165369, 0.33357874, 0.17797923, 0.38184963,\n",
       "         0.70966186, 0.6845619 , 0.41171032, 0.64226956, 0.07213422,\n",
       "         0.06055729, 0.11116644, 0.78329884, 0.95943339, 0.28852171,\n",
       "         0.69197801, 0.64966186, 0.03165361, 0.35779435, 0.04414077,\n",
       "         0.86690448, 0.88185956, 0.83721236, 0.79842189, 0.0415299 ,\n",
       "         0.05769569, 0.1190059 , 0.29648809, 0.45601879, 0.48057117,\n",
       "         0.93083847, 0.46561079, 0.70029198, 0.40791157, 0.08926672,\n",
       "         0.38245739, 0.17484976, 0.03507368, 0.07964614, 0.30874826,\n",
       "         0.21462646, 0.23522411, 0.11918239, 0.66743954, 0.90364617,\n",
       "         0.7649093 , 0.67219999, 0.15933609, 0.48423101, 0.3086462 ,\n",
       "         0.29619351, 0.71890697, 0.62045913, 0.0541583 , 0.51837171,\n",
       "         0.72639066, 0.07393632, 0.13308356, 0.04148721, 0.51731559,\n",
       "         0.2761123 , 0.18148926, 0.07974711, 0.27434203, 0.11225999,\n",
       "         0.48109716, 0.57088995, 0.37665923, 0.6345324 , 0.1222766 ,\n",
       "         0.45399772, 0.61943541, 0.45187015, 0.0605434 , 0.26288321,\n",
       "         0.05651653, 0.23795874, 0.64729101, 0.48542434, 0.44126202,\n",
       "         0.71889492, 0.25145161, 0.15229696, 0.48827848, 0.34073794,\n",
       "         0.82945887, 0.40035225, 0.09073991, 0.57548713, 0.24282347,\n",
       "         0.2985355 , 0.67477984, 0.12155494, 0.35731975, 0.3313852 ,\n",
       "         0.07996272, 0.21015135, 0.35456675, 0.22974372, 0.54156022,\n",
       "         0.18631143, 0.03835173, 0.70307281, 0.26245786, 0.77166057,\n",
       "         0.26697087, 0.15909884, 0.1515744 , 0.74187652, 0.17654048,\n",
       "         0.24386634, 0.31903943, 0.88684143, 0.21621035, 0.17798568,\n",
       "         0.48350072, 0.07944614, 0.94151566, 0.20510371, 0.04457196,\n",
       "         0.73648954, 0.56621477, 0.27408086, 0.77224334, 0.88928648,\n",
       "         0.16045169, 0.07661172, 0.00380806, 0.31789078, 0.39495484,\n",
       "         0.56104475, 0.3597763 , 0.20880196, 0.05549326, 0.01404014,\n",
       "         0.21481205, 0.32562764, 0.04996573, 0.06737391, 0.22535301,\n",
       "         0.74426603, 0.39785937, 0.92380795, 0.35075476, 0.86336075,\n",
       "         0.80370605, 0.65420746, 0.30989155, 0.75392789, 0.47022011,\n",
       "         0.24281229, 0.26770447, 0.03753524, 0.03566665, 0.21971532,\n",
       "         0.90523335, 0.0379946 , 0.0895923 , 0.16729733, 0.4280684 ,\n",
       "         0.80516864, 0.03729025, 0.12620786, 0.83099204, 0.24214405,\n",
       "         0.16064804, 0.03757054, 0.12318199, 0.10258963, 0.10320911,\n",
       "         0.09060344, 0.3464481 , 0.43649763, 0.50662165, 0.20547541,\n",
       "         0.12672194, 0.8630086 , 0.10066943, 0.12952441, 0.68299703,\n",
       "         0.39972633, 0.13917007, 0.25672762, 0.0313092 , 0.8250677 ,\n",
       "         0.12663452, 0.37147629, 0.43198074, 0.10805882, 0.72010149,\n",
       "         0.50012779, 0.23366822, 0.04701305, 0.92632233, 0.7469712 ,\n",
       "         0.27011652, 0.17831417, 0.60325174, 0.19970446, 0.35254567,\n",
       "         0.54029044, 0.12421393, 0.64557771, 0.02600515, 0.18664197,\n",
       "         0.37419355, 0.06605587, 0.21270865, 0.18593268, 0.81498349,\n",
       "         0.77470909, 0.01137648, 0.72336605, 0.32025208, 0.09844159,\n",
       "         0.09639969, 0.10385952, 0.04810418, 0.20132188, 0.10012083,\n",
       "         0.72373609, 0.74467672, 0.47172494, 0.02437813, 0.3489509 ,\n",
       "         0.72364981, 0.0782304 , 0.22800296, 0.39536998, 0.24162973,\n",
       "         0.99227423, 0.077457  , 0.09743011, 0.13907017, 0.13785157,\n",
       "         0.02419162, 0.28277959, 0.11738418, 0.41125576, 0.2231845 ,\n",
       "         0.91862449, 0.42927875, 0.08982488, 0.84937016, 0.58553928,\n",
       "         0.30792714, 0.02077461, 0.20377639, 0.08090053, 0.30912897,\n",
       "         0.09960163, 0.03110072, 0.15504343, 0.55434007, 0.83638241,\n",
       "         0.59916379, 0.26315834, 0.26311914, 0.41800246, 0.15692829,\n",
       "         0.24328256, 0.16879791, 0.16521979, 0.2732422 , 0.366948  ,\n",
       "         0.56140798, 0.19044028, 0.07391535, 0.06491397, 0.84448312,\n",
       "         0.41623295, 0.42744514, 0.92140521, 0.08500758, 0.89875197,\n",
       "         0.12945904, 0.0921466 , 0.14989689, 0.44093918, 0.00893811,\n",
       "         0.69797976, 0.14632363, 0.06035082, 0.80928708, 0.63468906,\n",
       "         0.08073233, 0.11859071, 0.02349765, 0.29544302, 0.18235412,\n",
       "         0.13997155, 0.67276313, 0.23843908, 0.12080088, 0.34302191,\n",
       "         0.21819817, 0.11107322, 0.14605343, 0.0758406 , 0.07227274,\n",
       "         0.53752468, 0.66586609, 0.5315326 , 0.23102271, 0.18379986,\n",
       "         0.01901339, 0.22876015, 0.04391115, 0.66593229, 0.25408915,\n",
       "         0.04472622, 0.02762877, 0.0973013 , 0.14083007, 0.11253535,\n",
       "         0.24945756, 0.35748017, 0.23590025, 0.29678775, 0.13774768,\n",
       "         0.57288805, 0.08564021, 0.02835283, 0.29558382, 0.44504301,\n",
       "         0.42137043, 0.29477537, 0.40779381, 0.10605402, 0.06687558,\n",
       "         0.84698458, 0.96092004, 0.298337  , 0.58573595, 0.73701329,\n",
       "         0.10472767, 0.09193381, 0.24822235, 0.06820592, 0.10720597,\n",
       "         0.20586557, 0.15425276, 0.27395951, 0.63906965, 0.17739115,\n",
       "         0.4182204 , 0.87577309, 0.11106434, 0.15849191, 0.08463866,\n",
       "         0.09559988, 0.16900017, 0.14935309, 0.52195324, 0.19171993,\n",
       "         0.07877369, 0.09451184, 0.17458978, 0.11843595, 0.29331856,\n",
       "         0.2871105 , 0.24054884, 0.43483521, 0.44654065, 0.92110364,\n",
       "         0.54179068, 0.14553483, 0.46184743, 0.3460226 , 0.31944687,\n",
       "         0.04759398, 0.64228427, 0.11242164, 0.84321642, 0.03774293,\n",
       "         0.81595368, 0.20885012, 0.41645957, 0.1751146 , 0.41814583,\n",
       "         0.6765681 , 0.10756277, 0.10433367, 0.68076703, 0.09682487,\n",
       "         0.07838974, 0.17248811, 0.13684235, 0.76317846, 0.85609996,\n",
       "         0.33317521, 0.85967413, 0.03520968, 0.48214083, 0.05893609,\n",
       "         0.14319915, 0.75608263, 0.83135984, 0.29156972, 0.76203068,\n",
       "         0.08762684, 0.16098145, 0.01490924, 0.51255059, 0.30360846,\n",
       "         0.19350245, 0.1515971 , 0.96104129, 0.16761202, 0.12121554,\n",
       "         0.14315229, 0.10132028, 0.22999418, 0.39129431, 0.05853674,\n",
       "         0.32533777, 0.09530925, 0.11612212, 0.10309374, 0.1402538 ,\n",
       "         0.42501373, 0.15921352, 0.10610489, 0.43421507, 0.0301739 ,\n",
       "         0.08506309, 0.34881315, 0.49908244, 0.23146255, 0.1239708 ,\n",
       "         0.49629571, 0.36812279, 0.76679607, 0.47287135, 0.0705292 ,\n",
       "         0.04460114, 0.23143409, 0.31326467, 0.19492315, 0.11010578,\n",
       "         0.51140125, 0.04653966, 0.46614646, 0.60836036, 0.15732647,\n",
       "         0.70266074, 0.96062283, 0.72327487, 0.76831425, 0.36634054,\n",
       "         0.13655566, 0.57098539, 0.27162706, 0.25080149, 0.63923497,\n",
       "         0.79862399, 0.07983625, 0.11232056, 0.7283863 , 0.36724572,\n",
       "         0.85628646, 0.56804996, 0.10548777, 0.32002845, 0.07081678,\n",
       "         0.01572825, 0.81235435, 0.20758368, 0.31546835, 0.07981031,\n",
       "         0.28454468, 0.16592901, 0.11664159, 0.22290491, 0.65413799,\n",
       "         0.23249262, 0.87150799, 0.4390656 , 0.61610772, 0.04245049,\n",
       "         0.30947559, 0.54442445, 0.10793766, 0.32859636, 0.62926354,\n",
       "         0.26263355, 0.36635249, 0.77422087, 0.67125486, 0.11006511,\n",
       "         0.15004582, 0.08097721, 0.24859282, 0.76384614, 0.18000749,\n",
       "         0.41428188, 0.32343765, 0.77275405, 0.14488828, 0.10106597,\n",
       "         0.90322656, 0.7709679 , 0.2078938 , 0.17858496, 0.2607099 ,\n",
       "         0.06436799, 0.20450064, 0.37865615, 0.365225  , 0.14742003,\n",
       "         0.33988409, 0.20573142, 0.28801066, 0.38045292, 0.07977737,\n",
       "         0.22845055, 0.22778409, 0.83387108, 0.11759936, 0.11486517,\n",
       "         0.18447613, 0.12163507, 0.1187914 , 0.16448267, 0.22641239,\n",
       "         0.76717464, 0.16782463, 0.09565869, 0.66091737, 0.92924191,\n",
       "         0.31217664, 0.69281453, 0.31132845, 0.82614966, 0.57279668,\n",
       "         0.54352397, 0.27592134, 0.10670918, 0.66567141, 0.72366832,\n",
       "         0.44174397, 0.48015936, 0.31866953, 0.16483335, 0.90090307,\n",
       "         0.09695624, 0.93467837, 0.08814316, 0.31900435, 0.31859379,\n",
       "         0.1714758 , 0.28563864, 0.0727153 ])}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_model(x: np.ndarray, y: np.ndarray, C: float) -> dict:\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    lr_model = LogisticRegression(max_iter=1000, C=C)\n",
    "    lr_model.fit(x, y)\n",
    "    lr_scores = cross_val_score(lr_model, x, y, cv=kf, scoring='precision')\n",
    "    lr_predict = lr_model.predict_proba(x)[:, 1]\n",
    "    lr_results = {\n",
    "        'mean': lr_scores.mean(),\n",
    "        'std': lr_scores.std(),\n",
    "        'predictions': lr_predict\n",
    "    }\n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"\\nLogistic Regression: Precision = {lr_results['mean']:.4f} ± {lr_results['std']:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'lr_results': lr_results\n",
    "    }\n",
    "\n",
    "run_model(X.values, y.values, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfabd3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Results:\n",
      "linear: precision = 0.7213 ± 0.1200\n",
      "rbf: precision = 0.7078 ± 0.0698\n",
      "poly: precision = 0.7450 ± 0.1440\n",
      "\n",
      "Logistic Regression: precision = 0.7234 ± 0.0996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'svm_results': {'linear': {'mean': 0.7213429444951184,\n",
       "   'std': 0.12000897514700935},\n",
       "  'rbf': {'mean': 0.7077548201861703, 'std': 0.06984649628593713},\n",
       "  'poly': {'mean': 0.7450161319588564, 'std': 0.1440226655095764}},\n",
       " 'lr_results': {'mean': 0.7234340103144787,\n",
       "  'std': 0.0995795351161728,\n",
       "  'predictions': array([0.71788557, 0.05004217, 0.7916649 , 0.04292874, 0.89864691,\n",
       "         0.14842183, 0.06808818, 0.63750014, 0.70997257, 0.03807967,\n",
       "         0.22122976, 0.89393557, 0.78146623, 0.6337848 , 0.62645407,\n",
       "         0.39738602, 0.37235677, 0.19810911, 0.35598437, 0.23576997,\n",
       "         0.39330902, 0.3185104 , 0.93786761, 0.29480507, 0.70010361,\n",
       "         0.44101334, 0.73329128, 0.04727481, 0.53929577, 0.27939413,\n",
       "         0.42647917, 0.5716599 , 0.05034582, 0.03780712, 0.43187454,\n",
       "         0.15193868, 0.66127902, 0.39334987, 0.1735713 , 0.57246524,\n",
       "         0.73887433, 0.6926571 , 0.11627473, 0.92515896, 0.62521318,\n",
       "         0.94990929, 0.43202883, 0.04098976, 0.37956448, 0.03986138,\n",
       "         0.03749065, 0.08672076, 0.06897989, 0.82526127, 0.70684618,\n",
       "         0.02349474, 0.88029203, 0.35837466, 0.82964008, 0.18471886,\n",
       "         0.01032847, 0.51924463, 0.02437444, 0.30859252, 0.35433509,\n",
       "         0.12041188, 0.19449076, 0.471383  , 0.03312359, 0.30275219,\n",
       "         0.19521642, 0.36390009, 0.81352824, 0.24856769, 0.05297115,\n",
       "         0.00210559, 0.07824708, 0.22300333, 0.66486304, 0.10174511,\n",
       "         0.10364212, 0.00616639, 0.13937028, 0.05135108, 0.67255807,\n",
       "         0.19385721, 0.52750281, 0.18851854, 0.78109638, 0.07584466,\n",
       "         0.02002706, 0.26047819, 0.33183473, 0.29813446, 0.25565044,\n",
       "         0.51530937, 0.08335021, 0.0180366 , 0.13542119, 0.45204381,\n",
       "         0.83257741, 0.28198402, 0.06418534, 0.03292025, 0.23208479,\n",
       "         0.24780365, 0.02042406, 0.42466657, 0.108202  , 0.09487402,\n",
       "         0.59775931, 0.70659129, 0.05542445, 0.0949544 , 0.73665918,\n",
       "         0.55325455, 0.35675488, 0.15928957, 0.11951508, 0.0541797 ,\n",
       "         0.88258148, 0.28450174, 0.15078459, 0.35608822, 0.13743818,\n",
       "         0.52305771, 0.45697679, 0.19609191, 0.19697618, 0.15746995,\n",
       "         0.63012116, 0.68074757, 0.66994572, 0.27331335, 0.05643361,\n",
       "         0.24395448, 0.09930611, 0.06930825, 0.26248807, 0.17866197,\n",
       "         0.1700225 , 0.32029731, 0.17346228, 0.38554744, 0.46604519,\n",
       "         0.00745898, 0.06541007, 0.30326581, 0.63018036, 0.04613325,\n",
       "         0.35311426, 0.13854645, 0.84290125, 0.5232295 , 0.95957638,\n",
       "         0.87656118, 0.08864848, 0.12628821, 0.05094959, 0.96894917,\n",
       "         0.42502866, 0.30307878, 0.22471488, 0.1052561 , 0.27893459,\n",
       "         0.23556719, 0.44472541, 0.3176704 , 0.23523254, 0.12503067,\n",
       "         0.15650991, 0.52115783, 0.19881975, 0.19118321, 0.05283464,\n",
       "         0.87028685, 0.12922758, 0.77310705, 0.7496119 , 0.6540884 ,\n",
       "         0.04743989, 0.26716085, 0.00218093, 0.05058131, 0.34639707,\n",
       "         0.94663077, 0.83761892, 0.37400458, 0.24399292, 0.35774735,\n",
       "         0.07849914, 0.48141979, 0.68436039, 0.96955705, 0.10101736,\n",
       "         0.6790186 , 0.06479299, 0.10986614, 0.34093155, 0.37335504,\n",
       "         0.17409539, 0.42805683, 0.1400063 , 0.0406062 , 0.33444317,\n",
       "         0.13219342, 0.94767339, 0.69242281, 0.09440086, 0.87272924,\n",
       "         0.05036594, 0.54997205, 0.83210529, 0.52209049, 0.31450793,\n",
       "         0.89465487, 0.30175257, 0.33382327, 0.17886049, 0.3816682 ,\n",
       "         0.70848132, 0.68383451, 0.41010137, 0.64180189, 0.07299386,\n",
       "         0.06130329, 0.1120406 , 0.78021311, 0.95887782, 0.28860256,\n",
       "         0.68939321, 0.64948089, 0.03222971, 0.35734703, 0.04479347,\n",
       "         0.8645643 , 0.88054969, 0.83472464, 0.79602816, 0.04221562,\n",
       "         0.05834496, 0.11987824, 0.29606923, 0.45556587, 0.48002338,\n",
       "         0.92952842, 0.46482273, 0.70031005, 0.40895783, 0.0901078 ,\n",
       "         0.38211727, 0.17568269, 0.03570694, 0.08039607, 0.3102506 ,\n",
       "         0.21479135, 0.23543343, 0.11992521, 0.66603805, 0.90231929,\n",
       "         0.76279015, 0.66868744, 0.15995989, 0.4844348 , 0.30870513,\n",
       "         0.29709797, 0.71530487, 0.61778994, 0.05477814, 0.51553943,\n",
       "         0.7242176 , 0.07467145, 0.13425142, 0.04211877, 0.51666327,\n",
       "         0.27619859, 0.18204183, 0.08064143, 0.27566703, 0.11352744,\n",
       "         0.47967141, 0.56981978, 0.37729929, 0.6331524 , 0.12368868,\n",
       "         0.45443118, 0.61984003, 0.45186674, 0.06129164, 0.26341363,\n",
       "         0.05728762, 0.23836485, 0.64583118, 0.48415882, 0.44149582,\n",
       "         0.71650579, 0.25294942, 0.15360111, 0.48845654, 0.34173925,\n",
       "         0.82638732, 0.3997098 , 0.09176508, 0.57379466, 0.24356536,\n",
       "         0.29888121, 0.67340186, 0.12254169, 0.35757595, 0.33199771,\n",
       "         0.08092507, 0.21090116, 0.35451315, 0.23015492, 0.54050007,\n",
       "         0.18711569, 0.03906771, 0.70054525, 0.26298064, 0.77005482,\n",
       "         0.26800079, 0.15975306, 0.15252191, 0.73998173, 0.17700936,\n",
       "         0.24461568, 0.31942736, 0.88483718, 0.21698502, 0.17900935,\n",
       "         0.4830015 , 0.08024003, 0.93991312, 0.20624001, 0.04521853,\n",
       "         0.73456735, 0.56390722, 0.2748989 , 0.77032133, 0.88747485,\n",
       "         0.16133965, 0.07766714, 0.00393297, 0.31814279, 0.39549091,\n",
       "         0.56058935, 0.35902642, 0.20846484, 0.05626024, 0.01440813,\n",
       "         0.21546203, 0.32568076, 0.05085543, 0.06815834, 0.22571015,\n",
       "         0.74252089, 0.39744159, 0.92190907, 0.35126021, 0.86160927,\n",
       "         0.80194813, 0.65315785, 0.31158685, 0.75267578, 0.47028515,\n",
       "         0.24325792, 0.26797602, 0.03809606, 0.0363126 , 0.22146246,\n",
       "         0.90407143, 0.0386588 , 0.09040442, 0.16799435, 0.42736251,\n",
       "         0.80436346, 0.0379578 , 0.12696744, 0.8285916 , 0.24329457,\n",
       "         0.16152461, 0.03814398, 0.12425132, 0.10350625, 0.10418802,\n",
       "         0.0913855 , 0.34633481, 0.43642346, 0.50779375, 0.20628267,\n",
       "         0.12820076, 0.86067828, 0.10209908, 0.13076001, 0.68072105,\n",
       "         0.3999619 , 0.14037828, 0.25658665, 0.03186851, 0.82233809,\n",
       "         0.12750189, 0.37189261, 0.43165492, 0.10910621, 0.71781179,\n",
       "         0.49875099, 0.23471203, 0.04767558, 0.924815  , 0.74619269,\n",
       "         0.27044185, 0.17938729, 0.60237747, 0.20027336, 0.35228437,\n",
       "         0.54043702, 0.12496493, 0.64356999, 0.02651953, 0.18749635,\n",
       "         0.37436311, 0.0668561 , 0.2130853 , 0.1863063 , 0.81320086,\n",
       "         0.77298836, 0.0116194 , 0.72155768, 0.32072517, 0.09988862,\n",
       "         0.09679414, 0.10507026, 0.04882894, 0.2020011 , 0.10116216,\n",
       "         0.72017613, 0.74246505, 0.47046389, 0.02483454, 0.34899055,\n",
       "         0.72182404, 0.07902352, 0.22855514, 0.39491495, 0.24194183,\n",
       "         0.99195052, 0.07841073, 0.09840796, 0.13980772, 0.13878422,\n",
       "         0.02469334, 0.28273441, 0.11853812, 0.411318  , 0.22362876,\n",
       "         0.91685459, 0.42958729, 0.09068894, 0.8480947 , 0.58593667,\n",
       "         0.30883988, 0.02120726, 0.20474214, 0.08186741, 0.3097514 ,\n",
       "         0.10045673, 0.03161769, 0.15589712, 0.55169126, 0.83418704,\n",
       "         0.59698516, 0.26302389, 0.26298128, 0.41832269, 0.1575547 ,\n",
       "         0.24475949, 0.17008318, 0.16636626, 0.27396428, 0.36800768,\n",
       "         0.56111002, 0.19118238, 0.07473025, 0.06581151, 0.84151805,\n",
       "         0.41604242, 0.42803789, 0.92043315, 0.08585348, 0.89744342,\n",
       "         0.13029798, 0.09329103, 0.15069441, 0.44119846, 0.00913969,\n",
       "         0.69687925, 0.14717442, 0.06118458, 0.80775213, 0.63359818,\n",
       "         0.08166272, 0.11940288, 0.02401845, 0.29611762, 0.18332219,\n",
       "         0.14110599, 0.67105154, 0.2390411 , 0.12155106, 0.34427613,\n",
       "         0.21916825, 0.11213769, 0.14753096, 0.0765799 , 0.07306256,\n",
       "         0.53608877, 0.66521794, 0.53101472, 0.23172691, 0.18650435,\n",
       "         0.01943698, 0.2295789 , 0.0443903 , 0.66413293, 0.25408009,\n",
       "         0.04532329, 0.02815104, 0.09827642, 0.14187848, 0.11346767,\n",
       "         0.24982197, 0.35698648, 0.23639805, 0.29597873, 0.13852716,\n",
       "         0.56948907, 0.08686221, 0.02908608, 0.29628171, 0.44500312,\n",
       "         0.42195383, 0.29545435, 0.40829841, 0.10704555, 0.06778773,\n",
       "         0.84538846, 0.96012739, 0.2988417 , 0.58486893, 0.73508714,\n",
       "         0.10543267, 0.0928822 , 0.25005667, 0.06895933, 0.1082223 ,\n",
       "         0.20725461, 0.15498428, 0.27526807, 0.63695629, 0.17831446,\n",
       "         0.41847796, 0.87400963, 0.11193899, 0.15938936, 0.08555337,\n",
       "         0.09644026, 0.16955736, 0.15071601, 0.5211194 , 0.19285608,\n",
       "         0.0797655 , 0.09542532, 0.17509067, 0.11932   , 0.29462825,\n",
       "         0.2867729 , 0.24129644, 0.43365537, 0.44567668, 0.91965408,\n",
       "         0.5397091 , 0.14617963, 0.46211464, 0.34626965, 0.32272394,\n",
       "         0.04820872, 0.64029577, 0.11328811, 0.84186849, 0.03819435,\n",
       "         0.81388451, 0.20963168, 0.41640545, 0.17598216, 0.41839473,\n",
       "         0.67471392, 0.10878339, 0.10484307, 0.6786186 , 0.09766786,\n",
       "         0.0792493 , 0.17256571, 0.13766505, 0.76180736, 0.85332034,\n",
       "         0.33236534, 0.85788622, 0.03580073, 0.48192657, 0.05980755,\n",
       "         0.14412674, 0.75415463, 0.83008029, 0.29162014, 0.76052893,\n",
       "         0.08848572, 0.16201892, 0.01524906, 0.51211327, 0.30243309,\n",
       "         0.19463719, 0.15253119, 0.96012132, 0.16841326, 0.1218988 ,\n",
       "         0.14415549, 0.10207191, 0.23032282, 0.39141769, 0.05919227,\n",
       "         0.32533613, 0.09624185, 0.11684401, 0.10422691, 0.1410753 ,\n",
       "         0.42448593, 0.1605052 , 0.10704206, 0.4337489 , 0.03076092,\n",
       "         0.08614528, 0.34824616, 0.49865305, 0.23122957, 0.12511396,\n",
       "         0.49677184, 0.36844407, 0.76411321, 0.47277205, 0.07121383,\n",
       "         0.04526608, 0.23201999, 0.31332486, 0.19539046, 0.11106275,\n",
       "         0.51171509, 0.04720155, 0.46641754, 0.6075943 , 0.15837365,\n",
       "         0.7011841 , 0.95945631, 0.72227766, 0.76643234, 0.36596248,\n",
       "         0.13757338, 0.57089413, 0.27209541, 0.25204219, 0.63791559,\n",
       "         0.7970432 , 0.080537  , 0.11381265, 0.72668742, 0.36859959,\n",
       "         0.85396426, 0.5673682 , 0.10625212, 0.31945124, 0.0718574 ,\n",
       "         0.01607574, 0.80964408, 0.20810861, 0.31545972, 0.08130382,\n",
       "         0.28529673, 0.16649164, 0.11735796, 0.22385394, 0.65313272,\n",
       "         0.23329659, 0.8698525 , 0.43810569, 0.61474588, 0.04307273,\n",
       "         0.31204595, 0.54323282, 0.10827776, 0.32924856, 0.62688182,\n",
       "         0.26352525, 0.36672114, 0.77248712, 0.66823058, 0.11109896,\n",
       "         0.15082389, 0.08152456, 0.24951709, 0.76178153, 0.18090359,\n",
       "         0.41480821, 0.32381968, 0.77016655, 0.14610633, 0.10202429,\n",
       "         0.90184399, 0.76903676, 0.20929302, 0.17946259, 0.26177413,\n",
       "         0.0652748 , 0.20533254, 0.37904086, 0.36582624, 0.14882436,\n",
       "         0.33987881, 0.20679052, 0.28795454, 0.37984771, 0.08045377,\n",
       "         0.22937812, 0.2282193 , 0.8316562 , 0.11854517, 0.1162649 ,\n",
       "         0.18495803, 0.1226872 , 0.11991063, 0.16536274, 0.22736522,\n",
       "         0.76566308, 0.1684925 , 0.09652961, 0.65950488, 0.92794432,\n",
       "         0.3132476 , 0.69031769, 0.31158577, 0.82430422, 0.57157116,\n",
       "         0.5415227 , 0.27628643, 0.10743857, 0.66526269, 0.72160442,\n",
       "         0.44174953, 0.47935448, 0.31947814, 0.16558767, 0.89963721,\n",
       "         0.09767876, 0.93323626, 0.08903045, 0.32079676, 0.31827591,\n",
       "         0.17251606, 0.28630598, 0.07344453])}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_model(x: np.ndarray, y: np.ndarray) -> dict:\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    svm_results = {}\n",
    "    for kernel in kernels:\n",
    "        svm_model = SVC(kernel=kernel, probability=True, max_iter=1000)\n",
    "        scores = cross_val_score(svm_model, x, y, cv=kf, scoring='precision')\n",
    "        svm_results[kernel] = {\n",
    "            'mean': scores.mean(),\n",
    "            'std': scores.std()\n",
    "        }\n",
    "\n",
    "    lr_model = LogisticRegression(max_iter=1000)\n",
    "    lr_model.fit(x, y)\n",
    "    lr_scores = cross_val_score(lr_model, x, y, cv=kf, scoring='precision')\n",
    "    lr_predict = lr_model.predict_proba(x)[:, 1]\n",
    "    lr_results = {\n",
    "        'mean': lr_scores.mean(),\n",
    "        'std': lr_scores.std(),\n",
    "        'predictions': lr_predict\n",
    "    }\n",
    "\n",
    "    print(\"SVM Results:\")\n",
    "    for kernel, result in svm_results.items():\n",
    "        print(f\"{kernel}: precision = {result['mean']:.4f} ± {result['std']:.4f}\")\n",
    "    print(\n",
    "        f\"\\nLogistic Regression: precision = {lr_results['mean']:.4f} ± {lr_results['std']:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'svm_results': svm_results,\n",
    "        'lr_results': lr_results\n",
    "    }\n",
    "\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "run_model(X.values, y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cdaf7c",
   "metadata": {},
   "source": [
    "- Regresión Logística:\n",
    "Obtuvo una precisión promedio de 0.7234 con una desviación estándar de ±0.0996. Su desempeño fue estable y consistente, lo que indica que el modelo se adapta bien sin sobreajustar los datos.\n",
    "\n",
    "- SVM con kernel lineal:\n",
    "Logró una precisión de 0.7213 ± 0.1200, muy similar a la regresión logística. Esto sugiere que la relación entre las variables y la presencia de diabetes puede modelarse de forma casi lineal.\n",
    "\n",
    "- SVM con kernel RBF:\n",
    "Alcanzó una precisión de 0.7078 ± 0.0698, ligeramente menor, aunque con menor variabilidad entre las iteraciones. Esto indica que el kernel RBF no aportó una mejora significativa para este problema.\n",
    "\n",
    "- SVM con kernel polinómico:\n",
    "Fue el modelo con mejor precisión promedio (0.7450 ± 0.1440), aunque presentó la desviación más alta. Esto significa que su rendimiento puede variar más dependiendo de la partición de los datos, mostrando cierto riesgo de sobreajuste.\n",
    "\n",
    "En general, todos los modelos tuvieron un buen desempeño, con precisiones alrededor del 70–75%, lo que muestra una capacidad predictiva aceptable. El SVM con kernel polinómico obtuvo la mejor precisión, pero su alta variabilidad lo hace menos estable. Por otro lado, la regresión logística resultó más equilibrada, sencilla y confiable, por lo que sería la opción más adecuada como modelo final para predecir la diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a254ca7",
   "metadata": {},
   "source": [
    "# Pregunta 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce491662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>sku</th>\n",
       "      <th>selling_price</th>\n",
       "      <th>original_price</th>\n",
       "      <th>currency</th>\n",
       "      <th>availability</th>\n",
       "      <th>color</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "      <th>source_website</th>\n",
       "      <th>breadcrumbs</th>\n",
       "      <th>description</th>\n",
       "      <th>brand</th>\n",
       "      <th>images</th>\n",
       "      <th>country</th>\n",
       "      <th>language</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>reviews_count</th>\n",
       "      <th>crawled_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.adidas.com/us/beach-shorts/FJ5089....</td>\n",
       "      <td>Beach Shorts</td>\n",
       "      <td>FJ5089</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Black</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>adidas United States</td>\n",
       "      <td>https://www.adidas.com</td>\n",
       "      <td>Women/Clothing</td>\n",
       "      <td>Splashing in the surf. Making memories with yo...</td>\n",
       "      <td>adidas</td>\n",
       "      <td>https://assets.adidas.com/images/w_600,f_auto,...</td>\n",
       "      <td>USA</td>\n",
       "      <td>en</td>\n",
       "      <td>4.5</td>\n",
       "      <td>35</td>\n",
       "      <td>2021-10-23 17:50:17.331255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.adidas.com/us/five-ten-kestrel-lac...</td>\n",
       "      <td>Five Ten Kestrel Lace Mountain Bike Shoes</td>\n",
       "      <td>BC0770</td>\n",
       "      <td>150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>adidas United States</td>\n",
       "      <td>https://www.adidas.com</td>\n",
       "      <td>Women/Shoes</td>\n",
       "      <td>Lace up and get after it. The Five Ten Kestrel...</td>\n",
       "      <td>adidas</td>\n",
       "      <td>https://assets.adidas.com/images/w_600,f_auto,...</td>\n",
       "      <td>USA</td>\n",
       "      <td>en</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-10-23 17:50:17.423830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.adidas.com/us/mexico-away-jersey/G...</td>\n",
       "      <td>Mexico Away Jersey</td>\n",
       "      <td>GC7946</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>InStock</td>\n",
       "      <td>White</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>adidas United States</td>\n",
       "      <td>https://www.adidas.com</td>\n",
       "      <td>Kids/Clothing</td>\n",
       "      <td>Clean and crisp, this adidas Mexico Away Jerse...</td>\n",
       "      <td>adidas</td>\n",
       "      <td>https://assets.adidas.com/images/w_600,f_auto,...</td>\n",
       "      <td>USA</td>\n",
       "      <td>en</td>\n",
       "      <td>4.9</td>\n",
       "      <td>42</td>\n",
       "      <td>2021-10-23 17:50:17.530834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.adidas.com/us/five-ten-hiangle-pro...</td>\n",
       "      <td>Five Ten Hiangle Pro Competition Climbing Shoes</td>\n",
       "      <td>FV4744</td>\n",
       "      <td>160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Black</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>adidas United States</td>\n",
       "      <td>https://www.adidas.com</td>\n",
       "      <td>Five Ten/Shoes</td>\n",
       "      <td>The Hiangle Pro takes on the classic shape of ...</td>\n",
       "      <td>adidas</td>\n",
       "      <td>https://assets.adidas.com/images/w_600,f_auto,...</td>\n",
       "      <td>USA</td>\n",
       "      <td>en</td>\n",
       "      <td>3.7</td>\n",
       "      <td>7</td>\n",
       "      <td>2021-10-23 17:50:17.615054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.adidas.com/us/mesh-broken-stripe-p...</td>\n",
       "      <td>Mesh Broken-Stripe Polo Shirt</td>\n",
       "      <td>GM0239</td>\n",
       "      <td>65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>adidas United States</td>\n",
       "      <td>https://www.adidas.com</td>\n",
       "      <td>Men/Clothing</td>\n",
       "      <td>Step up to the tee relaxed. This adidas golf p...</td>\n",
       "      <td>adidas</td>\n",
       "      <td>https://assets.adidas.com/images/w_600,f_auto,...</td>\n",
       "      <td>USA</td>\n",
       "      <td>en</td>\n",
       "      <td>4.7</td>\n",
       "      <td>11</td>\n",
       "      <td>2021-10-23 17:50:17.702680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.adidas.com/us/beach-shorts/FJ5089....   \n",
       "1  https://www.adidas.com/us/five-ten-kestrel-lac...   \n",
       "2  https://www.adidas.com/us/mexico-away-jersey/G...   \n",
       "3  https://www.adidas.com/us/five-ten-hiangle-pro...   \n",
       "4  https://www.adidas.com/us/mesh-broken-stripe-p...   \n",
       "\n",
       "                                              name     sku  selling_price  \\\n",
       "0                                     Beach Shorts  FJ5089             40   \n",
       "1        Five Ten Kestrel Lace Mountain Bike Shoes  BC0770            150   \n",
       "2                               Mexico Away Jersey  GC7946             70   \n",
       "3  Five Ten Hiangle Pro Competition Climbing Shoes  FV4744            160   \n",
       "4                    Mesh Broken-Stripe Polo Shirt  GM0239             65   \n",
       "\n",
       "  original_price currency availability  color  category                source  \\\n",
       "0            NaN      USD      InStock  Black  Clothing  adidas United States   \n",
       "1            NaN      USD      InStock   Grey     Shoes  adidas United States   \n",
       "2            NaN      USD      InStock  White  Clothing  adidas United States   \n",
       "3            NaN      USD      InStock  Black     Shoes  adidas United States   \n",
       "4            NaN      USD      InStock   Blue  Clothing  adidas United States   \n",
       "\n",
       "           source_website     breadcrumbs  \\\n",
       "0  https://www.adidas.com  Women/Clothing   \n",
       "1  https://www.adidas.com     Women/Shoes   \n",
       "2  https://www.adidas.com   Kids/Clothing   \n",
       "3  https://www.adidas.com  Five Ten/Shoes   \n",
       "4  https://www.adidas.com    Men/Clothing   \n",
       "\n",
       "                                         description   brand  \\\n",
       "0  Splashing in the surf. Making memories with yo...  adidas   \n",
       "1  Lace up and get after it. The Five Ten Kestrel...  adidas   \n",
       "2  Clean and crisp, this adidas Mexico Away Jerse...  adidas   \n",
       "3  The Hiangle Pro takes on the classic shape of ...  adidas   \n",
       "4  Step up to the tee relaxed. This adidas golf p...  adidas   \n",
       "\n",
       "                                              images country language  \\\n",
       "0  https://assets.adidas.com/images/w_600,f_auto,...     USA       en   \n",
       "1  https://assets.adidas.com/images/w_600,f_auto,...     USA       en   \n",
       "2  https://assets.adidas.com/images/w_600,f_auto,...     USA       en   \n",
       "3  https://assets.adidas.com/images/w_600,f_auto,...     USA       en   \n",
       "4  https://assets.adidas.com/images/w_600,f_auto,...     USA       en   \n",
       "\n",
       "   average_rating  reviews_count                  crawled_at  \n",
       "0             4.5             35  2021-10-23 17:50:17.331255  \n",
       "1             4.8              4  2021-10-23 17:50:17.423830  \n",
       "2             4.9             42  2021-10-23 17:50:17.530834  \n",
       "3             3.7              7  2021-10-23 17:50:17.615054  \n",
       "4             4.7             11  2021-10-23 17:50:17.702680  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('adidas.csv')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4995c8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 845 entries, 0 to 844\n",
      "Data columns (total 20 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   url             845 non-null    object \n",
      " 1   name            845 non-null    object \n",
      " 2   sku             845 non-null    object \n",
      " 3   selling_price   845 non-null    int64  \n",
      " 4   original_price  829 non-null    object \n",
      " 5   currency        845 non-null    object \n",
      " 6   availability    845 non-null    object \n",
      " 7   color           845 non-null    object \n",
      " 8   category        845 non-null    object \n",
      " 9   source          845 non-null    object \n",
      " 10  source_website  845 non-null    object \n",
      " 11  breadcrumbs     845 non-null    object \n",
      " 12  description     845 non-null    object \n",
      " 13  brand           845 non-null    object \n",
      " 14  images          845 non-null    object \n",
      " 15  country         845 non-null    object \n",
      " 16  language        845 non-null    object \n",
      " 17  average_rating  845 non-null    float64\n",
      " 18  reviews_count   845 non-null    int64  \n",
      " 19  crawled_at      845 non-null    object \n",
      "dtypes: float64(1), int64(2), object(17)\n",
      "memory usage: 132.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dbd4f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selling_price</th>\n",
       "      <th>original_price</th>\n",
       "      <th>availability</th>\n",
       "      <th>category</th>\n",
       "      <th>brand</th>\n",
       "      <th>country</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>reviews_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>25.0</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>adidas</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>25.0</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>adidas</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>25.0</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>adidas</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>80.0</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>adidas</td>\n",
       "      <td>USA</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>80.0</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>adidas</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>72</td>\n",
       "      <td>120.0</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>adidas</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>70</td>\n",
       "      <td>100.0</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>adidas</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>35</td>\n",
       "      <td>50.0</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>adidas</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>40</td>\n",
       "      <td>50.0</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>adidas</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>70</td>\n",
       "      <td>100.0</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>adidas</td>\n",
       "      <td>USA</td>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>829 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     selling_price  original_price availability  category   brand country  \\\n",
       "0               20            25.0      InStock  Clothing  adidas     USA   \n",
       "1               20            25.0      InStock  Clothing  adidas     USA   \n",
       "2               20            25.0      InStock  Clothing  adidas     USA   \n",
       "3               48            80.0      InStock  Clothing  adidas     USA   \n",
       "4               64            80.0      InStock     Shoes  adidas     USA   \n",
       "..             ...             ...          ...       ...     ...     ...   \n",
       "824             72           120.0      InStock     Shoes  adidas     USA   \n",
       "825             70           100.0      InStock     Shoes  adidas     USA   \n",
       "826             35            50.0      InStock     Shoes  adidas     USA   \n",
       "827             40            50.0      InStock     Shoes  adidas     USA   \n",
       "828             70           100.0      InStock     Shoes  adidas     USA   \n",
       "\n",
       "     average_rating  reviews_count  \n",
       "0                 1            116  \n",
       "1                 1            116  \n",
       "2                 1            116  \n",
       "3                 0            144  \n",
       "4                 1            160  \n",
       "..              ...            ...  \n",
       "824               1            151  \n",
       "825               1            135  \n",
       "826               1            190  \n",
       "827               1            190  \n",
       "828               1            135  \n",
       "\n",
       "[829 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('adidas.csv')\n",
    "data = data.drop(columns=['url', 'name', 'sku', 'description', 'images', 'source_website', 'source', 'breadcrumbs', 'language', 'currency', 'color', 'crawled_at'])\n",
    "data['original_price'] = data['original_price'].str.replace('$', '').astype(float)\n",
    "data['average_rating'] = data['average_rating'].apply(lambda x: 1 if x >= 4.3 else 0)\n",
    "num_col = ['selling_price', 'original_price', 'reviews_count']\n",
    "cat_col = ['availability', 'category', 'brand', 'country']\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "236735b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selling_price</th>\n",
       "      <th>original_price</th>\n",
       "      <th>reviews_count</th>\n",
       "      <th>availability_OutOfStock</th>\n",
       "      <th>category_Clothing</th>\n",
       "      <th>category_Shoes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>18</td>\n",
       "      <td>30.0</td>\n",
       "      <td>53</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>32</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1352</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>24</td>\n",
       "      <td>30.0</td>\n",
       "      <td>78</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>28</td>\n",
       "      <td>35.0</td>\n",
       "      <td>7291</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>44</td>\n",
       "      <td>55.0</td>\n",
       "      <td>177</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>48</td>\n",
       "      <td>60.0</td>\n",
       "      <td>214</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>28</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3812</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>52</td>\n",
       "      <td>65.0</td>\n",
       "      <td>671</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>44</td>\n",
       "      <td>55.0</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>24</td>\n",
       "      <td>30.0</td>\n",
       "      <td>78</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>580 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     selling_price  original_price  reviews_count  availability_OutOfStock  \\\n",
       "145             18            30.0             53                    False   \n",
       "306             32            35.0           1352                    False   \n",
       "234             24            30.0             78                    False   \n",
       "220             28            35.0           7291                    False   \n",
       "819             44            55.0            177                    False   \n",
       "..             ...             ...            ...                      ...   \n",
       "71              48            60.0            214                    False   \n",
       "106             28            35.0           3812                    False   \n",
       "270             52            65.0            671                    False   \n",
       "435             44            55.0             17                    False   \n",
       "102             24            30.0             78                    False   \n",
       "\n",
       "     category_Clothing  category_Shoes  \n",
       "145               True           False  \n",
       "306               True           False  \n",
       "234               True           False  \n",
       "220              False            True  \n",
       "819              False            True  \n",
       "..                 ...             ...  \n",
       "71                True           False  \n",
       "106              False            True  \n",
       "270               True           False  \n",
       "435              False            True  \n",
       "102               True           False  \n",
       "\n",
       "[580 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop(columns=['average_rating'])\n",
    "X = pd.get_dummies(X, columns=cat_col, drop_first=True)\n",
    "y = data['average_rating']\n",
    "x_test, x_train, y_test, y_train = train_test_split(X, y, train_size=0.7, random_state=42)\n",
    "\n",
    "x_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e1daf",
   "metadata": {},
   "source": [
    "#### Escalamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d5097f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(x_train[num_col])\n",
    "x_train[num_col] = scaler.transform(x_train[num_col])\n",
    "x_test[num_col] = scaler.transform(x_test[num_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5da4362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7683724509426701"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regresion = LogisticRegression()\n",
    "regresion.fit(x_train,y_train)\n",
    "y_pred = regresion.predict_proba(x_train)[:, 1]\n",
    "roc_auc = roc_auc_score(y_train, y_pred)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7efd421a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6790495049504951"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = regresion.predict_proba(x_test)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "roc_auc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc40ced6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249, 6)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = x_train.shape[0]\n",
    "p = x_train.shape[1]\n",
    "n,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0375570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto: = 3.0323, estadístico t = 27.5615, p-value = 0.0000e+00\n",
      "-> El intercepto es estadísticamente significativo.\n",
      "beta 1: -0.5052, estadístico t = -2.9998, p-value = 2.9838e-03\n",
      "-> El coeficiente beta 1 es estadísticamente significativo.\n",
      "beta 2: -0.2007, estadístico t = -1.1718, p-value = 2.4243e-01\n",
      "-> El coeficiente beta 2 no es estadísticamente significativo.\n",
      "beta 3: 0.8751, estadístico t = 25.8792, p-value = 0.0000e+00\n",
      "-> El coeficiente beta 3 es estadísticamente significativo.\n",
      "beta 4: 0.0000, estadístico t = 0.0000, p-value = 1.0000e+00\n",
      "-> El coeficiente beta 4 no es estadísticamente significativo.\n",
      "beta 5: -0.3411, estadístico t = -2.9060, p-value = 3.9996e-03\n",
      "-> El coeficiente beta 5 es estadísticamente significativo.\n",
      "beta 6: -0.5403, estadístico t = -4.2038, p-value = 3.6966e-05\n",
      "-> El coeficiente beta 6 es estadísticamente significativo.\n"
     ]
    }
   ],
   "source": [
    "n = x_train.shape[0]\n",
    "p = x_train.shape[1]\n",
    "\n",
    "RSS = np.sum((y_pred_test - y_test) ** 2)\n",
    "var = RSS / (n - p - 1)\n",
    "\n",
    "b_0 = np.ravel(regresion.intercept_)\n",
    "b_i = np.ravel(regresion.coef_)\n",
    "\n",
    "X = np.column_stack((np.ones(n), x_train))\n",
    "X = X.astype(float)\n",
    "\n",
    "var_beta = np.linalg.inv(X.T @ X + np.eye(X.shape[1]) * 1e-6) * var\n",
    "std_beta = np.sqrt(np.diag(var_beta))\n",
    "\n",
    "betas = np.concatenate((b_0, b_i))\n",
    "\n",
    "t_stats = betas / std_beta\n",
    "p_values = [2 * (1 - stats.t.cdf(np.abs(t), n - p - 1)) for t in t_stats]\n",
    "\n",
    "for i, (b, t, pval) in enumerate(zip(betas, t_stats, p_values)):\n",
    "    if i == 0:\n",
    "        print(f\"Intercepto: = {b:.4f}, estadístico t = {t:.4f}, p-value = {pval:.4e}\")\n",
    "        \n",
    "        if pval < 0.05:\n",
    "            print(\"-> El intercepto es estadísticamente significativo.\")\n",
    "        else:\n",
    "            print(\"-> El intercepto no es estadísticamente significativo.\")\n",
    "    else:\n",
    "        print(f\"beta {i}: {b:.4f}, estadístico t = {t:.4f}, p-value = {pval:.4e}\")\n",
    "\n",
    "        if pval < 0.05:\n",
    "            print(f\"-> El coeficiente beta {i} es estadísticamente significativo.\")\n",
    "        else:\n",
    "            print(f\"-> El coeficiente beta {i} no es estadísticamente significativo.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bb997db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercepto: 3.0323, t = 27.5615, p-value = 0.0000e+00\n",
      "Beta 1: -0.5052, t = -2.9998, p-value = 2.9838e-03\n",
      "Beta 2: -0.2007, t = -1.1718, p-value = 2.4243e-01\n",
      "Beta 3: 0.8751, t = 25.8792, p-value = 0.0000e+00\n",
      "Beta 4: 0.0000, t = 0.0000, p-value = 1.0000e+00\n",
      "Beta 5: -0.3411, t = -2.9060, p-value = 3.9996e-03\n",
      "Beta 6: -0.5403, t = -4.2038, p-value = 3.6966e-05\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "n = x_train.shape[0]\n",
    "p = x_train.shape[1]\n",
    "\n",
    "RSS = np.sum((y_pred_test - y_test) ** 2)\n",
    "var = RSS / (n - p - 1)\n",
    "\n",
    "b_0 = np.ravel(regresion.intercept_)\n",
    "b_i = np.ravel(regresion.coef_)\n",
    "\n",
    "X = np.column_stack((np.ones(n), x_train))\n",
    "X = X.astype(float)\n",
    "\n",
    "var_beta = np.linalg.inv(X.T @ X + np.eye(X.shape[1]) * 1e-6) * var\n",
    "std_beta = np.sqrt(np.diag(var_beta))\n",
    "\n",
    "betas = np.concatenate((b_0, b_i))\n",
    "\n",
    "t_stats = betas / std_beta\n",
    "p_values = [2 * (1 - stats.t.cdf(np.abs(t), n - p - 1)) for t in t_stats]\n",
    "\n",
    "for i, (b, t, pval) in enumerate(zip(betas, t_stats, p_values)):\n",
    "    if i == 0:\n",
    "        print(f\"Intercepto: {b:.4f}, t = {t:.4f}, p-value = {pval:.4e}\")\n",
    "    else:\n",
    "        print(f\"Beta {i}: {b:.4f}, t = {t:.4f}, p-value = {pval:.4e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0baefe",
   "metadata": {},
   "source": [
    "Intercepto (3.0323) → Significativo (p = 0.0000)\n",
    "\n",
    "Beta 1 (-0.5052) → Significativo (p = 0.00298)\n",
    "\n",
    "Beta 2 (-0.2007) → No significativo (p = 0.2424)\n",
    "\n",
    "Beta 3 (0.8751) → Significativo (p = 0.0000)\n",
    "\n",
    "Beta 4 (0.0000) → No significativo (p = 1.0000)\n",
    "\n",
    "Beta 5 (-0.3411) → Significativo (p = 0.0040)\n",
    "\n",
    "Beta 6 (-0.5403) → Significativo (p = 0.000037)\n",
    "\n",
    "\n",
    "El modelo logró explicar bien la relación entre el precio de venta y las demás variables del dataset. La mayoría de los coeficientes fueron estadísticamente significativos, lo que indica que sí aportan información relevante para predecir el precio.\n",
    "\n",
    "Los betas significativos (1, 3, 5 y 6) influyen de forma importante en la variable objetivo, mientras que los no significativos (2 y 4) podrían eliminarse o revisarse porque no aportan valor estadístico claro.\n",
    "\n",
    "El modelo muestra un desempeño moderado, con un ROC-AUC de 0.77 en entrenamiento y 0.68 en prueba, lo que sugiere que generaliza decentemente, aunque pierde algo de precisión fuera de la muestra.\n",
    "\n",
    "En general, el modelo es útil pero puede mejorarse ajustando las variables menos relevantes o probando técnicas de regularización o selección de características."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
